\chapter{Аналитический раздел}

Сжатие иногда называют исходным кодированием, поскольку оно пытается удалить избыточность в строке из-за предсказуемости источника.
Для конкретной изображения коэффициент сжатия - это отношение размера сжатого выхода к исходному размеру изображения. \cite{text-compression}

Когда человечество столкнулось с необходимостью сжатия, начали появляться первые алгоритмы.
Они реализовывали сжатие без потери какой-либо информации.
Однако в последнее время стали появляться новые классы изображений, сжатие над которыми ранее существовавшими алгоритмами стало неэффективным -- они почти не сжимались, хотя обладали явной избыточностью. \cite{troubles-rating-while-compressing}

Тогда возникла необходимость использования алгоритмов с потерями.

Алгоритмы сжатия можно разделить на 2 главных класса: без потерь (англ. lossless) и с потерями (англ. lossy). 
Данные, сжатые с использованием первого подхода могут быть восстановлены точь-в-точь, когда сжатие с потерями подразумевает потерю некоторых данных. \cite{sayood2002lossless}

\textbf{TODO: классифицировать алгоритмы сжатия.}

Процесс сжатия данных состоит из 2 этапов:
\begin{itemize}
    \item моделирование;
    \item кодирование.
\end{itemize}

\section{Моделирование}

На этом этапе происходит разбиение входных данных по частоте появления. Наиболее частые последовательности битов обладают большим значением вероятности и наоборот.
Таким образом, на данном этапе происходит присвоение вероятностей для последовательностей битов. 

\section{Кодирование}

Зная длину кода, соответствующую данной последовательности в зависимости от ее частоты (\ref{entropy}), необходимо составить уникальную цепочку символов, которая будет единым образом сопоставлять исходную последовательность с кодированной.

\textbf{TODO:}

\section{Связь моделирования и кодирования}

Отношение между этими этапами было установлено в теореме Шеннона: символ, вероятность появления которого равна $p$ будет представлен \begin{math}-\log{p}\end{math} битами.
В таком случае, высокочастотный символ будет закодирован минимальным количеством битов, а низкочастотный --- максимальным. Тогда длину последовательности символов можно определить с помощью формулы
\begin{equation}\label{entropy}
   -\displaystyle\sum_{i=1}^{N}p_{i}\log{p},
\end{equation}
где $N$ --- количество последовательностей, $p_{i}$ --- вероятность $i$-й последовательности.

Эта величина называется \textit{энтропией} вероятностного распределения.

\section{LZSS}

LZSS, описанный Сторером (англ. Storer) и Шимански (англ. Szymanski”), является предком алгоритма LZ77 (Lempel-Ziv-77). 
Он использует метод динамического словаря.
Метод синтаксического анализа, используемый LZSS, является жадным (без предварительного просмотра).

Словарь на каждом шаге состоит из всех односимвольных подстрок и всех подстрок взодных данных, которые начинаются с уже сжатого префикса входных данных и заканчиваются перед концом анализируемой фразы.
Каждое кодовое слово состоит из битового флага, который обозначает, относится ли кодовое слово к типу односимвольных подстрок или всех подстрок.
Кодовые слова первого типа --- односимвольных фраз --- просто состоят из соответствующего символа.
Кодовые слова второго типа состоят из двух элементов: указателя на начальное местоположение последнего вхождения фразы и длины фразы.
Как указатель, так и информация о длине могут быть представлены фиксированным числом битов. \cite{sayood2002lossless}

\textbf{Применение.} Утилита $gzip$, распространяемая Free Software Foundation, использует модификацию алгоритма LZSS.

\section{LZW}

LZW использует жадный синтаксический анализ как и LZ77.
Разница заключается в том, что изначально словарь состоит только из односимвольных подстрок и после каждого шага синтаксического анализа проанализированная фраза объединяется с первым символом несжатой части входных данных и вставляется в словарь как новая фраза.
Каждой такой фразе присваивается $|D| + 1$ в качестве кодового слова, где $|D|$ --- количество фраз в словаре.
В таком случае на любом шаге каждая фраза представлена $\log_2{|D|}$ битами.

\textbf{Применение.} Алгоритм LZW с небольшими изменениями в обслуживании словаря используется в GIF.

На листингах \ref{lst:lzw-encoding}--\ref{lst:lzw-decoding} представлен псевдокод алгоритма LZW.

\section{Проблема переполнения динамического словаря}

Важной проблемой при реализации методов Lempel~---~Ziv, основанных на работе с динамическим словарем, является ограничение памяти в словаре.
Есть несколько способов борьбы с этим явлением.

\begin{enumerate}
    \item \textbf{Ограничение размера.} 
    При достижении максимальной вместимости происходит пересоздание словаря.
    \item \textbf{Замораживание добавления.}
    При этом подходе в момент достижения ограничения приостанавливается вставка новых фраз, а дальнейшее сжатие реализуется без каких-либо изменений.
    \item \textbf{Удаление наименее недавно использованных (англ. least recently used --- LRU).}
    В данном случае происходит удаление пары фраза/кодовое слово из словаря при условии, что она использовалась наиболее давно из всего набора.
    Ключевое слово может быть переиспользовано для новой фразы, вставляемой далее.
    Этот подход требует хранения времени последнего использования каждой фразы.
    В этом заключается главный минус подхода.
    \item \textbf{Удаление наименее часто попадающихся фраз/кодовых слов (англ. least frequently used --- LFU).}
    Частота фразы определяется как количество обращений к ней, нормализованное относительно количества обращений ко всему словарю с момента вставки этой фразы/кодового слова.
    Такой подход требует обновления информации о частоте каждой фразы на каждом шаге.
\end{enumerate}

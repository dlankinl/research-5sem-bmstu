\chapter{Аналитический раздел}

Сжатие иногда называют исходным кодированием, поскольку оно пытается удалить избыточность в строке из-за предсказуемости источника.
Для конкретной изображения коэффициент сжатия - это отношение размера сжатого выхода к исходному размеру изображения. \cite{text-compression}

Когда человечество столкнулось с необходимостью сжатия, начали появляться первые алгоритмы.
Они реализовывали сжатие без потери какой-либо информации.
Однако в последнее время стали появляться новые классы изображений, сжатие над которыми ранее существовавшими алгоритмами стало неэффективным -- они почти не сжимались, хотя обладали явной избыточностью. \cite{troubles-rating-while-compressing}

Тогда возникла необходимость использования алгоритмов с потерями.

\textbf{TODO: что такое алгоритм с потерями и без потерь.}

\textbf{TODO: классифицировать алгоритмы сжатия.}

Процесс сжатия данных состоит из 2 этапов:
\begin{itemize}
    \item моделирование;
    \item кодирование.
\end{itemize}

\section{Моделирование}

На этом этапе происходит разбиение входных данных по частоте появления. Наиболее частые последовательности битов обладают большим значением вероятности и наоборот.
Таким образом, на данном этапе происходит присвоение вероятностей для последовательностей битов. 

\section{Кодирование}

Зная длину кода, соответствующую данной последовательности в зависимости от ее частоты (\ref{entropy}), необходимо составить уникальную цепочку символов, которая будет единым образом сопоставлять исходную последовательность с кодированной.

\textbf{TODO:}

\section{Связь моделирования и кодирования}

Отношение между этими этапами было установлено в теореме Шеннона: символ, вероятность появления которого равна $p$ будет представлен \begin{math}-\log{p}\end{math} битами.
В таком случае, высокочастотный символ будет закодирован минимальным количеством битов, а низкочастотный --- максимальным. Тогда длину последовательности символов можно определить с помощью формулы
\begin{equation}\label{entropy}
   -\displaystyle\sum_{i=1}^{N}p_{i}\log{p},
\end{equation}
где $N$ --- количество последовательностей, $p_{i}$ --- вероятность $i$-й последовательности.

Эта величина называется \textit{энтропией} вероятностного распределения.